{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# AUHack 2023 - Grundfos Hands-on ML Workshop - Building Type Classification\n",
    "The goal of this workshop is to introduce you to how and what real-world data science is.\n",
    "\n",
    "This workshop is based on an internal Data Hackthon, where the goal was to classify the type of building based on iGRID heat meter data.\n",
    "\n",
    "As this is a hands-on workshop .....\n",
    "\n",
    "Something about pragmatic => lost of exploring not too much ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'seaborn'\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 100x100 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# database connector\n",
    "import snowflake.connector\n",
    "\n",
    "# tidyverse tools\n",
    "import plotnine as p9\n",
    "from plotnine import ggplot, aes, geom_point, geom_histogram, facet_wrap, theme, ggtitle\n",
    "\n",
    "import patchworklib as pw\n",
    "\n",
    "import siuba\n",
    "from siuba import _, select, mutate, group_by, ungroup, filter, summarize\n",
    "from siuba.dply.vector import lead, lag\n",
    "\n",
    "# todo: where should the imports be? It feels bad to have them all here, but it also feels cluttered to have the all over the place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 120)\n",
    "\n",
    "p9.options.set_option('dpi', 300)\n",
    "p9.options.set_option('figure_size', (8, 6))\n",
    "# todo: test with codespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "We load the data from a Snowflake database using the `snowflake-connector-python` package and its `fetch_pandas_all()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "snowflake_user = ''\n",
    "snowflake_password = ''\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    account='da84422.west-europe.azure',\n",
    "    #account='iw47870.west-europe.azure',\n",
    "    user=snowflake_user,\n",
    "    password=snowflake_password,\n",
    "    database='GF_PROD_DB',\n",
    "    schema='CURATED_HACKATHON',\n",
    "    #warehouse='DATA_SCIENTIST_WH',\n",
    "    #role='GRP_DATA_SCIENTIST_ROLE',\n",
    "    )\n",
    "\n",
    "cur = conn.cursor()\n",
    "try:\n",
    "    cur.execute(\"select * from GF_PROD_DB.CURATED_HACKATHON.V_DATA;\")\n",
    "    heat_data=cur.fetch_pandas_all()\n",
    "    print('data:')\n",
    "    print(heat_data.head(3))\n",
    "    cur.execute(\"select * from GF_PROD_DB.CURATED_HACKATHON.V_METADATA;\")\n",
    "    metadata=cur.fetch_pandas_all()\n",
    "    print('\\nMetadata:')\n",
    "    print(metadata.head(3))\n",
    "finally:\n",
    "    cur.close()\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCISE 1 SOLUTION\n",
    "heat_data.info()\n",
    "metadata.info()\n",
    "print(f'\\nLOCATION_ELEVATION is missing for {len(metadata) - metadata.LOCATION_ELEVATION.nunique()} out of {len(metadata)} buildings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's merge the heat meter data and metadata\n",
    "# NOTE: In the real world, I would also split data here, but for simplicity we do that later.\n",
    "data = metadata.merge(heat_data, how='left', on='METER_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data engineering & exploration: Engineering part\n",
    "The goal of this section is to give an introduction to the first steps a Data Scientist takes to understand new data. We want the data to be understandable, usable, and trustworthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# select a meter_id to look at\n",
    "meter_id_to_plot = metadata.sample(n=1)['METER_ID'].iloc[0]\n",
    "print(f'We are investigating METER_ID = {meter_id_to_plot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we need to select a single METER_ID to get a comprehensible plot.\n",
    "data_to_plot = (data >> filter(_.METER_ID == meter_id_to_plot)).reset_index()\n",
    "\n",
    "# Let's have a look at the four values in df_train_meter\n",
    "p1 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='ENERGY')) + geom_point())\n",
    "p2 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='VOLUME')) + geom_point())\n",
    "p3 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='FORWARD_TEMPERATURE_CUMULATIVE')) + geom_point())\n",
    "p4 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='RETURN_TEMPERATURE_CUMULATIVE')) + geom_point())\n",
    "p = (p1 | p2) / (p3 | p4)\n",
    "p.savefig() # This is an artifact from Patchworklib, and does not save anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# They are all cumulative, so let's convert energy, volume and the temperatures to delta values.\n",
    "data = (\n",
    "    data >>\n",
    "    group_by('METER_ID') >>\n",
    "    mutate(\n",
    "        TIME_DELTA = _.TIMESTAMP - lag(_.TIMESTAMP, n=1, default=np.NaN),\n",
    "        ENERGY_DELTA = _.ENERGY - lag(_.ENERGY, n=1, default=None),\n",
    "        VOLUME_DELTA = _.VOLUME - lag(_.VOLUME, n=1, default=None),\n",
    "        FORWARD_TEMPERATURE_DELTA = _.FORWARD_TEMPERATURE_CUMULATIVE - lag(_.FORWARD_TEMPERATURE_CUMULATIVE, n=1, default=None),\n",
    "        RETURN_TEMPERATURE_DELTA = _.RETURN_TEMPERATURE_CUMULATIVE - lag(_.RETURN_TEMPERATURE_CUMULATIVE, n=1, default=None)\n",
    "    ) >>\n",
    "    ungroup()\n",
    ")\n",
    "\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we need to select a single METER_ID to get a comprehensible plot.\n",
    "data_to_plot = (data >> filter(_.METER_ID == meter_id_to_plot)).reset_index()\n",
    "\n",
    "# Let's have a look at the four values in df_train_meter\n",
    "p1 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='ENERGY_DELTA')) + geom_point())\n",
    "p2 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='VOLUME_DELTA')) + geom_point())\n",
    "p3 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='FORWARD_TEMPERATURE_DELTA')) + geom_point())\n",
    "p4 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='RETURN_TEMPERATURE_DELTA')) + geom_point())\n",
    "p = (p1 | p2) / (p3 | p4)\n",
    "p.savefig() # This is an artifact from Patchworklib, and does not save anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The measurements are *supposed* to be daily, but let's make a sanity check\n",
    "(\n",
    "    ggplot(data >> filter(-np.isnat(_.TIME_DELTA )), aes('TIME_DELTA')) + geom_histogram(bins=40, fill='#e66066', color='black') +\n",
    "    p9.scale_y_log10() +\n",
    "    ggtitle('Distribution of time between measurements')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# lets make the daily estimates of each column.\n",
    "data = (\n",
    "    data >>\n",
    "    mutate(\n",
    "        ENERGY_DAILY = _.ENERGY_DELTA * (pd.Timedelta(hours=24) / _.TIME_DELTA),\n",
    "        VOLUME_DAILY = _.VOLUME_DELTA * (pd.Timedelta(hours=24) / _.TIME_DELTA),\n",
    "        FORWARD_TEMPERATURE_DAILY = _.FORWARD_TEMPERATURE_DELTA * (pd.Timedelta(hours=24) / _.TIME_DELTA),\n",
    "        RETURN_TEMPERATURE_DAILY = _.RETURN_TEMPERATURE_DELTA * (pd.Timedelta(hours=24) / _.TIME_DELTA),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's compare the 'energy_delta' and the 'energy_daily'.\n",
    "data_to_plot = (data >> filter(_.METER_ID == meter_id_to_plot)).reset_index()\n",
    "\n",
    "p1 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='ENERGY_DELTA')) + geom_point() + ggtitle('This plot shows the non-normalised values'))\n",
    "p2 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='ENERGY_DAILY')) + geom_point() + ggtitle('This plot shows the normalised values with respect to the time gap'))\n",
    "p = (p1 | p2)\n",
    "p.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ... and similar comparison for 'forward_temperature'.\n",
    "p1 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='FORWARD_TEMPERATURE_DELTA')) + geom_point() + ggtitle('This plot shows the non-normalised values'))\n",
    "p2 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='FORWARD_TEMPERATURE_DAILY')) + geom_point() + ggtitle('This plot shows the normalised values with respect to the time gap'))\n",
    "p = (p1 | p2)\n",
    "p.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# lets convert forward and return temperatures to Celcius.\n",
    "data = (\n",
    "    data >>\n",
    "    mutate(\n",
    "        FORWARD_TEMPERATURE_CELCIUS_DAILY = _.FORWARD_TEMPERATURE_DAILY / _.VOLUME_DAILY,\n",
    "        RETURN_TEMPERATURE_CELCIUS_DAILY = _.RETURN_TEMPERATURE_DAILY / _.VOLUME_DAILY,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The temperatures should probably be between 0 and 100 degress celcius. Lets make another sanity check\n",
    "(\n",
    "    ggplot(data) +\n",
    "    geom_histogram(aes('FORWARD_TEMPERATURE_CELCIUS_DAILY'), bins=40, fill='red', color='black', alpha=0.6) +\n",
    "    geom_histogram(aes('RETURN_TEMPERATURE_CELCIUS_DAILY'), bins=40, fill='blue', color='black', alpha=0.6) +\n",
    "    p9.scale_y_log10() +\n",
    "    ggtitle('Distribution of forward and return temperatures, respectively.')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCISE 2:\n",
    "# * Create a new column, `TEMPERATURE_DIFFERENCE_CELCIUS_DAILY`, which show the difference between the Forward and the Return temperature.\n",
    "# * Create a plot showing `FORWARD_TEMPERATURE_CELCIUS_DAILY` in red and `RETURN_TEMPERATURE_CELCIUS_DAILY` in blue.\n",
    "# HINT: Add two `geom_points()` to the same ggplot. See: https://plotnine.readthedocs.io/en/stable/generated/plotnine.geoms.geom_point.html\n",
    "# * Create a plot showing the new column `TEMPERATURE_DIFFERENCE_CELCIUS_DAILY`.\n",
    "# * Combine the two plots using Patchworklib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCISE 2 SOLUTION\n",
    "\n",
    "# Let's calculate the temperature difference\n",
    "data = (\n",
    "    data >>\n",
    "    mutate(TEMPERATURE_DIFFERENCE_CELCIUS_DAILY = _.FORWARD_TEMPERATURE_CELCIUS_DAILY - _.RETURN_TEMPERATURE_CELCIUS_DAILY)\n",
    ")\n",
    "\n",
    "# Let`s plot the three temperatures\n",
    "data_to_plot = (data >> filter(_.METER_ID == meter_id_to_plot)).reset_index()\n",
    "\n",
    "p1 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='ENERGY_DELTA')) + geom_point() + ggtitle('This plot shows the non-normalised values'))\n",
    "p2 = pw.load_ggplot(ggplot(data_to_plot, aes(x='TIMESTAMP', y='ENERGY_DAILY')) + geom_point() + ggtitle('This plot shows the normalised values with respect to the time gap'))\n",
    "p = (p1 | p2)\n",
    "p.savefig()\n",
    "\n",
    "\n",
    "p1 = pw.load_ggplot(\n",
    "    ggplot(data_to_plot, aes(x='TIMESTAMP')) +\n",
    "    geom_point(aes(y='FORWARD_TEMPERATURE_CELCIUS_DAILY'), color='red') +\n",
    "    geom_point(aes(y='RETURN_TEMPERATURE_CELCIUS_DAILY'), color='blue') +\n",
    "    theme(axis_text_x = p9.element_text(rotation=90, hjust=0.35)) +\n",
    "    ggtitle('The forward (red) and return (blue) temperatures.')\n",
    ")\n",
    "p2 = pw.load_ggplot(\n",
    "    ggplot(data_to_plot, aes(x='TIMESTAMP',y='TEMPERATURE_DIFFERENCE_CELCIUS_DAILY')) +\n",
    "    geom_point() +\n",
    "    theme(axis_text_x = p9.element_text(rotation=90, hjust=0.35)) +\n",
    "    ggtitle('The difference between forward and return temperature.')\n",
    ")\n",
    "p = (p1 | p2)\n",
    "p.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data engineering & exploration: Exploration part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Before considering features, let's see the distrubution of our target; `BUILDING_TYPE`\n",
    "(\n",
    "    ggplot(metadata, aes('BUILDING_TYPE', fill='BUILDING_TYPE')) +\n",
    "    p9.geom_bar(color='black') +\n",
    "    ggtitle('The counts of residential and non-residentail meters included in the dataset')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's consider the metadata features and start with temperature difference.\n",
    "(\n",
    "    ggplot(data, aes('TEMPERATURE_DIFFERENCE_CELCIUS_DAILY', fill='BUILDING_TYPE')) +\n",
    "    p9.geom_histogram(bins=30, color='black') +\n",
    "    p9.xlim(-5, 60) +\n",
    "    ggtitle('The distribution of the temperature differences (for all meters)')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ... given the uneven distribution of residential and non-residential buildings, the above is hard to decipher. Lets split the plot.\n",
    "(\n",
    "    ggplot(data, aes('TEMPERATURE_DIFFERENCE_CELCIUS_DAILY', fill='BUILDING_TYPE')) +\n",
    "    p9.geom_histogram(bins=30, color='black') +\n",
    "    p9.xlim(-5, 60) +\n",
    "    facet_wrap('~BUILDING_TYPE', scales='free_y') +\n",
    "    ggtitle('The distribution of the temperature differences (for all meters)')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ... and repeat this for ENERGY_DAILY.\n",
    "(\n",
    "    ggplot(data, aes('ENERGY_DAILY', fill='BUILDING_TYPE')) +\n",
    "    p9.geom_histogram(bins=100, color='black') +\n",
    "    p9.xlim(0, 1) +\n",
    "    facet_wrap('~BUILDING_TYPE', scales='free_y') +\n",
    "    ggtitle('The distribution of the daily energy consumption (for all meters)')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at a couple of the metadata variables.\n",
    "(\n",
    "    ggplot(metadata, aes('BUILT_UPON_AREA', fill='BUILDING_TYPE')) +\n",
    "    p9.geom_histogram(bins=40, color='black') +\n",
    "    p9.xlim(0, 2000) +\n",
    "    facet_wrap('~BUILDING_TYPE', scales='free_y') +\n",
    "    ggtitle('The distribution of the built upon area (for all meters)')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCISE 3:\n",
    "# * Make a histogram plot of the `LOCATION_ELEVATION` split by `BUILDING_TYPE`\n",
    "# * What does this result tell us?\n",
    "# * Why is this a surprising signal (and what won't it generalize beyond this dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCISE 3 SOLUTION\n",
    "\n",
    "# Let's look at the `LOCATION_ELEVATION`\n",
    "(\n",
    "    ggplot(metadata, aes('LOCATION_ELEVATION', fill='BUILDING_TYPE')) +\n",
    "    p9.geom_histogram(bins=55, color='black') +\n",
    "    p9.xlim(0, 55) +\n",
    "    facet_wrap('~BUILDING_TYPE', scales='free_y') +\n",
    "    ggtitle('The distribution of the built upon area (for all meters)')\n",
    ")\n",
    "# Answer: We can see that the non-residential building are closer to the sea level in this city. This is unlikely to generalize to other cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Feature engineering\n",
    "The goal of this section is extract features from the time series that can be used in our models.\n",
    "\n",
    "At the end of this section we will have a pruned dataset ready to use for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's calculate some features from the daily energy usage\n",
    "features = (\n",
    "    data >>\n",
    "    group_by('METER_ID') >>\n",
    "    summarize(\n",
    "        ENERGY_DAILY_MEAN = _.ENERGY_DAILY.mean(),\n",
    "        ENERGY_DAILY_MEDIAN = _.ENERGY_DAILY.median(),\n",
    "        ENERGY_DAILY_CV = _.ENERGY_DAILY.std() / _.ENERGY_DAILY.mean(),\n",
    "        ENERGY_DAILY_AUTOCORR = _.ENERGY_DAILY.autocorr(),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's add the BUILDING_TYPE and visualize.\n",
    "features = metadata.merge(features, how='left', on='METER_ID')\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at a couple of the metadata variables.\n",
    "# NOTE: We use geom_density here as there are (relatively) few datapoints/rows.\n",
    "p1 = pw.load_ggplot(\n",
    "    ggplot(features, aes('ENERGY_DAILY_MEAN', fill='BUILDING_TYPE')) +\n",
    "    p9.geom_density(alpha=0.5) +\n",
    "    p9.xlim(0, 1.5) +\n",
    "    ggtitle('The density of the Mean of the daily energy consumption')\n",
    ")\n",
    "\n",
    "p2 = pw.load_ggplot(\n",
    "    ggplot(features, aes('ENERGY_DAILY_MEDIAN', fill='BUILDING_TYPE')) +\n",
    "    p9.geom_density(alpha=0.5) +\n",
    "    p9.xlim(0, 1.5) +\n",
    "    ggtitle('The density of the Median of the daily energy consumption')\n",
    ")\n",
    "\n",
    "p3 = pw.load_ggplot(\n",
    "    ggplot(features, aes('ENERGY_DAILY_CV', fill='BUILDING_TYPE')) +\n",
    "    p9.geom_density(alpha=0.5) +\n",
    "    p9.xlim(0, 4) +\n",
    "    ggtitle('The density of the Coefficient of Variance of the daily energy consumption')\n",
    ")\n",
    "\n",
    "p4 = pw.load_ggplot(\n",
    "    ggplot(features, aes('ENERGY_DAILY_AUTOCORR', fill='BUILDING_TYPE')) +\n",
    "    p9.geom_density(alpha=0.5) +\n",
    "    p9.xlim(0, 1) +\n",
    "    ggtitle('The density of the Autocorrelation of the daily energy consumption')\n",
    ")\n",
    "\n",
    "p = (p1 | p2) / (p3 | p4)\n",
    "p.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCISE 4:\n",
    "# * Calculate `Mean` and `Coefficient of Variance` for daily Energy, Volume, Forward Temperature (celcius), Return Temperature (celcius), and Temperature Difference (celcius).\n",
    "# HINT: overwrite the features DataFrame.\n",
    "# * Calculate another feature!\n",
    "# HINT: you can find inspiration here: https://pandas.pydata.org/docs/reference/api/pandas.Series.describe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCISE 4 SOLUTION\n",
    "\n",
    "from numpy import mean, absolute\n",
    "\n",
    "features = (\n",
    "    data >>\n",
    "    group_by('METER_ID') >>\n",
    "    summarize(\n",
    "        ENERGY_DAILY_MEAN = _.ENERGY_DAILY.mean(),\n",
    "        ENERGY_DAILY_MEDIAN = _.ENERGY_DAILY.median(),\n",
    "        ENERGY_DAILY_CV = _.ENERGY_DAILY.std() / _.ENERGY_DAILY.mean(),\n",
    "        ENERGY_DAILY_AUTOCORR = _.ENERGY_DAILY.autocorr(),\n",
    "        ENERGY_DAILY_MAD = mean(absolute(_.ENERGY_DAILY - mean(_.ENERGY_DAILY))),\n",
    "        VOLUME_DAILY_MEAN = _.VOLUME_DAILY.mean(),\n",
    "        VOLUME_DAILY_CV = _.VOLUME_DAILY.std() / _.VOLUME_DAILY.mean(),\n",
    "        FORWARD_TEMPERATURE_CELCIUS_DAILY_MEAN = _.FORWARD_TEMPERATURE_CELCIUS_DAILY.mean(),\n",
    "        FORWARD_TEMPERATURE_CELCIUS_DAILY_CV = _.FORWARD_TEMPERATURE_CELCIUS_DAILY.std() / _.FORWARD_TEMPERATURE_CELCIUS_DAILY.mean(),\n",
    "        RETURN_TEMPERATURE_CELCIUS_DAILY_MEAN = _.RETURN_TEMPERATURE_CELCIUS_DAILY.mean(),\n",
    "        RETURN_TEMPERATURE_CELCIUS_DAILY_CV = _.RETURN_TEMPERATURE_CELCIUS_DAILY.std() / _.RETURN_TEMPERATURE_CELCIUS_DAILY.mean(),\n",
    "        TEMPERATURE_DIFFERENCE_CELCIUS_DAILY_MEAN = _.TEMPERATURE_DIFFERENCE_CELCIUS_DAILY.mean(),\n",
    "        TEMPERATURE_DIFFERENCE_CELCIUS_DAILY_CV = _.TEMPERATURE_DIFFERENCE_CELCIUS_DAILY.std() / _.TEMPERATURE_DIFFERENCE_CELCIUS_DAILY.mean(),\n",
    "        ENERGY_DAILY_Q25 = _.ENERGY_DAILY.quantile(q=0.25),\n",
    "        ENERGY_DAILY_Q75 = _.ENERGY_DAILY.quantile(q=0.75),\n",
    "        TEMPERATURE_DIFFERENCE_CELCIUS_DAILY_MIN = _.TEMPERATURE_DIFFERENCE_CELCIUS_DAILY.min(),\n",
    "        TEMPERATURE_DIFFERENCE_CELCIUS_DAILY_MAX = _.TEMPERATURE_DIFFERENCE_CELCIUS_DAILY.max(),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create our dataset for modelling. First lets remove unnecessary columns.\n",
    "data_final  = (\n",
    "    metadata >>\n",
    "    select(- _.contains('UNIT')) >>\n",
    "    select(- _.TIMESTAMP_TIMEZONE) >>\n",
    "    select(- _.METER_TYPE)\n",
    ").merge(features, how='left', on='METER_ID')\n",
    "\n",
    "data_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_final = data_final.fillna(0)\n",
    "float64_cols = list(data_final.select_dtypes(include='float64'))\n",
    "data_final[float64_cols] = data_final[float64_cols].astype('float32')\n",
    "\n",
    "data_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modeling\n",
    "The goal of this section is train and test a simple model to predict the BUILDING_TYPE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First we split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "metadata_slim = (\n",
    "    metadata >>\n",
    "    select(- _.contains('UNIT')) >>\n",
    "    select(- _.TIMESTAMP_TIMEZONE) >>\n",
    "    select(- _.METER_TYPE)\n",
    ")\n",
    "\n",
    "train, test = train_test_split(metadata_slim, test_size = 0.25, stratify=metadata_slim['BUILDING_TYPE'])\n",
    "#train, test = train_test_split(data_final, test_size = 0.25, stratify=data_final['BUILDING_TYPE'])\n",
    "print(f'Number of train examples: {len(train)} out of {len(data_final)}.')\n",
    "print(f'Number of test examples: {len(test)} out of {len(data_final)}.')\n",
    "\n",
    "X_train = (train >> select(- _.BUILDING_TYPE)).fillna(0).to_numpy()\n",
    "y_train = (train >> select(_.BUILDING_TYPE)).fillna(0).to_numpy()\n",
    "X_test = (test >> select(- _.BUILDING_TYPE)).fillna(0).to_numpy()\n",
    "y_test = (test >> select(_.BUILDING_TYPE)).fillna(0).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Lets train a decision tree\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}